{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def file_to_list(filename='./SMSSpamCollection.txt'):\n",
    "    \"\"\"\n",
    "    将.txt文件内容转换成列表\n",
    "    Args:\n",
    "        filename: 文件路径\n",
    "\n",
    "    Returns:\n",
    "        class_list: 邮件类别\n",
    "        conten_list: 邮件内容\n",
    "    \"\"\"\n",
    "    fr = open(filename)\n",
    "    array_of_lines = fr.readlines()\n",
    "    class_list = [] # 存放每封邮件的类别--spam or ham\n",
    "    content_list = [] # 存放每封邮件的内容\n",
    "    token_list = [] # 存放处理后的内容所划分出的词条\n",
    "    for line in array_of_lines: # 取出每封邮件的类别与内容\n",
    "        content = line.strip('\\n').split('\\t')\n",
    "        # class_list.append(content[0])\n",
    "        if content[0] == 'ham':\n",
    "            class_list.append(0)\n",
    "        elif content[0] == 'spam':\n",
    "            class_list.append(1)\n",
    "        else:\n",
    "            class_list.append(-1)\n",
    "        del(content[0])\n",
    "        content_list.extend(content)\n",
    "    for content in content_list: # 划分词条\n",
    "        token_list.append(content_to_token(content))\n",
    "    return class_list, token_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def content_to_token(content):\n",
    "    \"\"\"\n",
    "    使用正则表达式, 将邮件内容划分成若干个词条, 用列表进行存储\n",
    "    Args:\n",
    "        content: 邮件内容\n",
    "\n",
    "    Returns:\n",
    "        token: 词条\n",
    "    \"\"\"\n",
    "    word_list = re.split('\\W', content) # 正则表达式'\\W': 匹配字母, 数字, 下划线或汉字, 等价于'[^A-Za-z0-9_]'\n",
    "    token = [word.lower() for word in word_list if len(word) > 0] # 去除掉长度小于等于2的字符串,如空格,\"a\"等\n",
    "    return token"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def create_vocab_list(dataset):\n",
    "    \"\"\"\n",
    "    创建词汇表, 存储了训练集所有出现过的词汇\n",
    "    Args:\n",
    "        dataset: 数据集\n",
    "\n",
    "    Returns:\n",
    "        list(vocab_set): 词汇表\n",
    "    \"\"\"\n",
    "    vocab_set = set([])\n",
    "    for document in dataset:\n",
    "        vocab_set = vocab_set | set(document)\n",
    "    return list(vocab_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def token_to_word_vector_euqal_weight(vocab_list, token):\n",
    "    \"\"\"\n",
    "    词集模型\n",
    "    \"\"\"\n",
    "    word_vector = [0] * len(vocab_list)\n",
    "    for word in token:\n",
    "        if word in vocab_list:\n",
    "            word_vector[vocab_list.index(word)] = 1\n",
    "    return word_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def token_to_word_vector(vocab_list, token):\n",
    "    \"\"\"\n",
    "    词袋模型\n",
    "    \"\"\"\n",
    "    word_vector = [0] * len(vocab_list)\n",
    "    for word in token:\n",
    "        if word in vocab_list:\n",
    "            word_vector[vocab_list.index(word)] += 1\n",
    "    return word_vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def train_NB(train_matrix, train_category):\n",
    "    num_train = len(train_matrix)\n",
    "    num_words = len(train_matrix[0])\n",
    "    pAbusive = sum(train_category) / float(num_train)\n",
    "    # p0_num = np.zeros(num_words)\n",
    "    # p1_num = np.zeros(num_words)\n",
    "    # p0_denom = 0.\n",
    "    # p1_denom = 0.\n",
    "    # laplace平滑, 避免除0异常\n",
    "    p0_num = np.ones(num_words)\n",
    "    p1_num = np.ones(num_words)\n",
    "    # p0_denom = 2.\n",
    "    # p1_denom = 2.\n",
    "    p0_denom = float(num_words)\n",
    "    p1_denom = float(num_words)\n",
    "    for i in range(num_train):\n",
    "        if train_category[i] == 1: # spam\n",
    "            p1_num += train_matrix[i]\n",
    "            p1_denom += sum(train_matrix[i])\n",
    "        else:\n",
    "            p0_num += train_matrix[i]\n",
    "            p0_denom += sum(train_matrix[i])\n",
    "    # p0_vector = p0_num / p0_denom\n",
    "    # p1_vecotr = p1_num / p1_denom\n",
    "    # 避免下溢出\n",
    "    p0_vector = np.log(p0_num / p0_denom) # 后验概率\n",
    "    p1_vecotr = np.log(p1_num / p1_denom) # 后验概率\n",
    "    return p0_vector, p1_vecotr, pAbusive"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def train_test_split(class_list, word_vector_list, proportion=0.7, random_seed=None):\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = word_vector_list\n",
    "    test_label = class_list\n",
    "    train_size = len(word_vector_list) * proportion\n",
    "    random.seed(random_seed)\n",
    "    while len(train_data) < train_size:\n",
    "        train_data_idx = random.randrange(len(test_data))\n",
    "        train_data.append(test_data.pop(train_data_idx))\n",
    "        train_label.append(test_label.pop(train_data_idx))\n",
    "    return train_data, train_label, test_data, test_label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def classify_NB(word_vector, p0_vector, p1_vector, pAbusive):\n",
    "    p1 = sum(word_vector * p1_vector) + np.log(pAbusive)\n",
    "    p0 = sum(word_vector * p0_vector) + np.log(1. - pAbusive)\n",
    "    prob = []\n",
    "    prob.append(p0)\n",
    "    prob.append(p1)\n",
    "    prob = np.array(prob)\n",
    "    if p1 > p0:\n",
    "        return 1, prob\n",
    "    else:\n",
    "        return 0, prob"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def testing_NB(test_data, test_category, p0_vector, p1_vector, pAbusive):\n",
    "    pred = []\n",
    "    score = []\n",
    "    for i in range(len(test_data)):\n",
    "        pred_tmp, score_tmp = classify_NB(test_data[i], p0_vector, p1_vector, pAbusive)\n",
    "        pred.append(pred_tmp)\n",
    "        score.append(score_tmp)\n",
    "    return pred, score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def calculate_metrics(pred, test_category):\n",
    "    # spam--1, ham--0\n",
    "    true_positive = 0.\n",
    "    false_positive = 0.\n",
    "    false_negative = 0.\n",
    "    true_negative = 0.\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i] == test_category[i]:\n",
    "            if pred[i] == 1:\n",
    "                true_positive += 1.\n",
    "            else:\n",
    "                true_negative += 1.\n",
    "        else:\n",
    "            if pred[i] == 1:\n",
    "                false_positive += 1.\n",
    "            else:\n",
    "                false_negative += 1.\n",
    "    accuracy = (true_positive + true_negative) / len(pred)\n",
    "    precision = (true_positive) / (true_positive + false_positive)\n",
    "    recall = (true_positive) / (true_positive + false_negative)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return accuracy, precision, recall, f1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "class_list, token_list = file_to_list()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# 所有单词构成的集合\n",
    "vocab_list = create_vocab_list(token_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# 多项式\n",
    "word_vector_list = []\n",
    "for token in token_list:\n",
    "    word_vector_list.append(token_to_word_vector(vocab_list, token))\n",
    "train_data, train_label, test_data, test_label = train_test_split(class_list, word_vector_list, random_seed=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# # 伯努利模型\n",
    "# word_vector_list = []\n",
    "# for token in token_list:\n",
    "#     word_vector_list.append(token_to_word_vector_euqal_weight(vocab_list, token))\n",
    "# train_data, train_label, test_data, test_label = train_test_split(class_list, word_vector_list, random_seed=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# # 混合模型\n",
    "# train_data_tmp, train_label, test_data_tmp, test_label = train_test_split(class_list, token_list, random_seed=4)\n",
    "# train_data = []\n",
    "# test_data = []\n",
    "# for train_token in train_data_tmp:\n",
    "#     train_data.append(token_to_word_vector(vocab_list, train_token))\n",
    "# for test_token in test_data_tmp:\n",
    "#     test_data.append(token_to_word_vector_euqal_weight(vocab_list, test_token))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "p0_vector, p1_vector, pAbusive = train_NB(train_data, train_label)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 预测\n",
    "pred, scores = testing_NB(test_data, test_label, p0_vector, p1_vector, pAbusive)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.98, Precison:0.926, Recall:0.938, F1:0.932\n"
     ]
    }
   ],
   "source": [
    "accuracy, precision, recall, f1 = calculate_metrics(pred, test_label)\n",
    "print('Accuracy:{:.3}, Precison:{:.3}, Recall:{:.3}, F1:{:.3}'.format(accuracy, precision, recall, f1))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
